{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a300fa97-41b6-449e-ac22-b90d13aa392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.57.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (10.3.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.20.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (7.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, bitsandbytes\n",
      "\u001b[2K  Attempting uninstall: sympy\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [bitsandbytes][0m [bitsandbytes]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bitsandbytes-0.49.0 sympy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate pillow torch torchvision bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d54fc9-b6f3-48e2-9dc4-e0bec61c3a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 09:28:05.679330: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_748/979427407.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  resnet.load_state_dict(torch.load(\"outputs/resnet_best.pt\", map_location=device))\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2360281b1a4e30b8681ff3d0843c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-Klasse: Datenvorschau\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Datenvorschau'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 235\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# 7. Beispielaufruf\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/workspace/Projekt/Testdaten/Data/Ungelabelt/14fdd34fd6b2427384abc61636ec44bc.png\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 224\u001b[39m, in \u001b[36mprocess_image\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m    221\u001b[39m pred_class, img = classify_image(image_path)\n\u001b[32m    222\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResNet-Klasse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m prompt = \u001b[43mPROMPTS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred_class\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    225\u001b[39m json_output = qwen_extract(img, prompt)\n\u001b[32m    227\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mJSON-Ausgabe:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'Datenvorschau'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. ResNet18 laden\n",
    "# -------------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "num_classes = 13\n",
    "class_names = [\n",
    "    \"ABAP Dictionary\",\n",
    "    \"BW4Cockpit (Stammdaten)\",\n",
    "    \"Bewegungsdaten\",\n",
    "    \"Composite Provider\",\n",
    "    \"DTP\",\n",
    "    \"Data Flow Object\",\n",
    "    \"Data Mart\",\n",
    "    \"Data Source\",\n",
    "    \"Data Store Object\",\n",
    "    \"Datenvorschau\",\n",
    "    \"Excel\",\n",
    "    \"Query\",\n",
    "    \"Transformationen\"\n",
    "]\n",
    "\n",
    "resnet = models.resnet18(weights=None)\n",
    "resnet.fc = torch.nn.Linear(resnet.fc.in_features, num_classes)\n",
    "resnet.load_state_dict(torch.load(\"outputs/resnet_best.pt\", map_location=device))\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Qwen laden\n",
    "# -------------------------------------------------------\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=\"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\")\n",
    "\n",
    "qwen = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    token=\"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\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Bildvorverarbeitung für ResNet\n",
    "# -------------------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def classify_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = resnet(tensor)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "    return class_names[pred.item()], img\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Prompts pro Klasse\n",
    "# -------------------------------------------------------\n",
    "PROMPTS = {\n",
    "    \"Excel\": \"\"\"\n",
    "Extrahiere die tabellarischen Daten aus dem Excel-Screenshot.\n",
    "Ignoriere UI-Elemente wie Menüleisten, Filterbereiche, Metadaten oder Seitentitel.\n",
    "Konzentriere dich ausschließlich auf die sichtbare Tabelle.\n",
    "\n",
    "Gib NUR gültiges JSON zurück.\n",
    "KEIN Markdown.\n",
    "KEINE ```json Blöcke.\n",
    "KEINE Erklärungen.\n",
    "KEINE Kommentare.\n",
    "KEINE zusätzlichen Texte.\n",
    "\n",
    "Format:\n",
    "\n",
    "{\n",
    "  \"columns\": [\"Spalte1\", \"Spalte2\", ...],\n",
    "  \"rows\": [\n",
    "    [\"Wert1\", \"Wert2\", ...],\n",
    "    [\"Wert1\", \"Wert2\", ...]\n",
    "  ]\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "    \"DTP\": \"\"\"\n",
    "Analysiere den Screenshot eines Data Transfer Process (DTP) in SAP BW/4HANA.\n",
    "\n",
    "Extrahiere ausschließlich die für einen DTP relevanten Informationen:\n",
    "\n",
    "- DTP-Name bzw. technische ID\n",
    "- Ausführungsmodus (z. B. Serial SAP HANA Execution, Dialog, Hintergrund)\n",
    "- Quelle des DTP (z. B. DataSource, RSDS, Datei)\n",
    "- Zielobjekt (z. B. ADSO, InfoCube)\n",
    "- Sichtbare Prozessschritte (z. B. Fill data transfer intermediate storage, Prepare for Extraction, Data Package Loop)\n",
    "- Hinweise auf die Extraktionsart (z. B. Datei-Extraktion)\n",
    "- Sichtbare Systemmeldungen oder Pop-ups (z. B. SAP GUI Security File Access)\n",
    "\n",
    "Gib NUR gültiges JSON zurück.\n",
    "KEIN Markdown.\n",
    "KEINE ```json Blöcke.\n",
    "KEINE Erklärungen.\n",
    "KEINE Kommentare.\n",
    "KEINE zusätzlichen Texte.\n",
    "\n",
    "Format:\n",
    "\n",
    "{\n",
    "  \"dtp_name\": \"\",\n",
    "  \"execution_mode\": \"\",\n",
    "  \"source\": \"\",\n",
    "  \"target\": \"\",\n",
    "  \"process_steps\": [],\n",
    "  \"extraction_type\": \"\",\n",
    "  \"system_messages\": []\n",
    "}\n",
    "\"\"\",\n",
    "\n",
    "        \"Transformationen\": \"\"\"\n",
    "Analysiere den Screenshot einer Transformation in SAP BW/4HANA.\n",
    "\n",
    "Extrahiere ausschließlich die variablen, inhaltlichen Informationen der Transformation:\n",
    "\n",
    "- Name bzw. technische ID der Quelle\n",
    "- Name bzw. technische ID des Ziels\n",
    "- Alle Quellfelder (Name + Datentyp)\n",
    "- Alle Zielfelder (Name + Datentyp)\n",
    "- Alle sichtbaren Feldzuordnungen (source_field → target_field)\n",
    "- Nur echte Mappings, keine Linieninterpretation\n",
    "- Behandle auch farbige, dünne oder schwach sichtbare Linien als gültige Mappings, wenn sie eine Verbindung zwischen zwei Feldern darstellen.\n",
    "- Keine automatisch generierten Felder erfinden\n",
    "\n",
    "Gib NUR gültiges JSON zurück.\n",
    "KEIN Markdown.\n",
    "KEINE ```json Blöcke.\n",
    "KEINE Erklärungen.\n",
    "KEINE Kommentare.\n",
    "KEINE zusätzlichen Texte.\n",
    "\n",
    "Format:\n",
    "\n",
    "{\n",
    "  \"source\": {\n",
    "    \"name\": \"\",\n",
    "    \"fields\": [\n",
    "      {\"name\": \"\", \"type\": \"\"}\n",
    "    ]\n",
    "  },\n",
    "  \"target\": {\n",
    "    \"name\": \"\",\n",
    "    \"fields\": [\n",
    "      {\"name\": \"\", \"type\": \"\"}\n",
    "    ]\n",
    "  },\n",
    "  \"mappings\": [\n",
    "    {\"source_field\": \"\", \"target_field\": \"\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Qwen JSON-Extraktion (KORREKT!)\n",
    "# -------------------------------------------------------\n",
    "def qwen_extract(image, prompt):\n",
    "    print(\"[QWEN] Baue messages...\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"[QWEN] Wende chat template an...\")\n",
    "    text_prompt = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    print(\"[QWEN] Erzeuge Inputs...\")\n",
    "    inputs = processor(\n",
    "        text=text_prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    for k, v in inputs.items():\n",
    "        print(f\"[QWEN] {k} shape:\", tuple(v.shape))\n",
    "\n",
    "    print(\"[QWEN] Starte generate()...\")\n",
    "    output = qwen.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,   # jetzt aktiv\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        eos_token_id=None      # jetzt aktiv\n",
    "    )\n",
    "    print(\"[QWEN] generate() fertig.\")\n",
    "\n",
    "    decoded = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    print(\"[QWEN] Decoding fertig.\")\n",
    "    return decoded\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Pipeline ausführen\n",
    "# -------------------------------------------------------\n",
    "def process_image(image_path):\n",
    "    pred_class, img = classify_image(image_path)\n",
    "    print(f\"ResNet-Klasse: {pred_class}\")\n",
    "\n",
    "    prompt = PROMPTS[pred_class]\n",
    "    json_output = qwen_extract(img, prompt)\n",
    "\n",
    "    print(\"\\nJSON-Ausgabe:\")\n",
    "    print(json_output)\n",
    "    return json_output\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Beispielaufruf\n",
    "# -------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    process_image(\"/workspace/Projekt/Testdaten/Data/Ungelabelt/58c4a60002ea47ce84086a31e8410242.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b6a8c-38c7-406b-8516-ad2c37a93613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1b1ce-ceb8-4628-816a-738f9091c6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9d531-3c60-4f40-9b1b-78759d84c9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
